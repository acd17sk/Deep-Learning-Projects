{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab8PzAEDav2V"
   },
   "source": [
    "# Learn to Write Like Shakespeare\n",
    "\n",
    "In this assignment we will implement a simple recurrent network with one hidden layer.\n",
    "We train this network on a medium-size poem \"The Sonnet\" written by William Shakespeare and use it for auto-completing sentences/phrases.\n",
    "\n",
    "For training the network, we will need to transform the text into something machine-processable.\n",
    "Basically, for each of the characters in the text, we provide a $D$-element one-hot encoding vector, where D is the total number of unique characters in the dataset.\n",
    "Character sequences of length $S$ will, hence, be turned into matrices of size $\\mathbf X = \\{\\vec x^{\\{s\\}}, 1 \\leq s\\leq S\\} \\in \\mathbb R^{S\\times D}$.\n",
    "For each input, we provide the target values $\\mathbf T$ of the same size, where the target for each sample is the next character: $\\vec t^{\\{s\\}} = \\vec x ^{\\{s+1\\}}$.\n",
    "\n",
    "To speed up processing, these sequences will be put into batches, i.e., $\\mathcal X, \\mathcal T \\in \\mathbb R^{B\\times S\\times D}$.\n",
    "This will automatically be achieved using the default PyTorch `DataLoader`.\n",
    "\n",
    "The data that we will use is originally provided here: http://raw.githubusercontent.com/brunoklein99/deep-learning-notes/master/shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9BBTl5pav2X"
   },
   "source": [
    "## Data and Targets Preprocessing\n",
    "\n",
    "First, we need to load the whole dataset $\\vec c \\in \\mathbb R^N$, a vector of characters, and turn the data sequence into one-hot encodings.\n",
    "For this purpose, we need to know the number $D$ of unique characters in our text.\n",
    "For simplicity, we only consider lower-case characters and special symbols such as punctuation marks.\n",
    "Also, the newline characters `'\\n'` need to be handled -- you can also leave them inside and see what happens.\n",
    "\n",
    "Then, for each of the characters, we need to assign a one-hot encoding, and build sequences of encodings.\n",
    "For a given index $n$ into our data and a given sequence length $S$, we provide the input $\\mathbf X ^{[n]}$ and the target $\\mathbf T^{[n]}$ as follows:\n",
    "\n",
    "\n",
    "  $$\\mathbf X^{[n]} = \\{\\mathrm{enc}(n-S+s-1) | 1 \\leq s \\leq S\\}$$\n",
    "  $$\\mathbf T^{[n]} = \\{\\mathrm{enc}(n-S+s) | 1 \\leq s \\leq S\\}$$\n",
    "\n",
    "where $\\mathrm{enc}$ is a function that returns the one-hot encoding for the character at the specified location in the original dataset $\\vec c$.\n",
    "In the case that the computation ($n-S+s-1$ or $n-S+s$) results in a negative value $\\vec 0$ should be used instead.\n",
    "\n",
    "For example, for the original text `abcde`, sequence length $S=7$ and index $n=4$, we would have the representations for $x = $ `000abcd` and $t=$ `00abcde`.\n",
    "\n",
    "Finally, we implement our own `Dataset` that returns the input and target encodings for any element of our input text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZEGOa7Nav2X"
   },
   "source": [
    "### Download the data file\n",
    "\n",
    "Please run the code block below to download the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KU7Itrg0av2X"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# download the data file\n",
    "filename = \"shakespeare.txt\"\n",
    "if not os.path.exists(filename):\n",
    "  url = \"http://raw.githubusercontent.com/brunoklein99/deep-learning-notes/master/\"\n",
    "  import urllib.request\n",
    "  urllib.request.urlretrieve(url+filename, 'data/' + filename)\n",
    "  print (\"Downloaded datafile\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRdPZmX9av2Y"
   },
   "source": [
    "### Set up of the device to run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztl8m0tkav2Y"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXVOEImwav2Z"
   },
   "source": [
    "### Task 1: Data Characteristics\n",
    "\n",
    "Implement a function that:\n",
    "1. Loads all text data from the poem file `shakespeare.txt`, iterates through and collect all the lowercase data that we want to learn from.\n",
    "2. Create a list of unique characters contained in our data. This will allow us to obtain the dimension $D$.\n",
    "\n",
    "Note:\n",
    "\n",
    "* Here, we consider only lowercase characters to reduce the alphabet size.\n",
    "* Please make sure that you handle the newline character at the end of each line consistently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVnHXoETav2Z"
   },
   "outputs": [],
   "source": [
    "def get_data(datafile='shakespeare.txt'):\n",
    "    # 1. Read the entire file into a single string\n",
    "    with open(datafile) as f:\n",
    "        full_text = f.read()\n",
    "\n",
    "    # 2. Process the text: convert to lowercase and replace newlines with asterisks\n",
    "    processed_text = full_text.lower().replace('\\n', '*')\n",
    "\n",
    "    # 3. The 'data' list is simply a list of all characters in the processed text\n",
    "    data = list(processed_text)\n",
    "\n",
    "    # 4. The 'characters' list is the sorted set of unique characters\n",
    "    #    This guarantees a consistent order every time.\n",
    "    characters = sorted(list(set(processed_text)))\n",
    "\n",
    "    return data, characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JueFBDS2av2Z",
    "outputId": "92686a3c-0f23-48a6-e4a2-daa1cd48f245"
   },
   "outputs": [],
   "source": [
    "data, characters = get_data(datafile='data/shakespeare.txt')\n",
    "\n",
    "D = len(characters)\n",
    "print (f\"Collected a total of {len(data)} elements of {D} unique characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZBwKEaaav2Z"
   },
   "source": [
    "### Task 2: One-hot Encoding\n",
    "\n",
    "Implement a dictionary that provides a unique one-hot encoding for each of the characters in the dataset.\n",
    "The dictionary takes as:\n",
    "\n",
    "1. the key a character\n",
    "2. its value is its one-hot vector representation of dimension $D$\n",
    "\n",
    "Each of the characters need to be represented by a one-hot encoding.\n",
    "Create a dictionary that provides the encoding for each unique character.\n",
    "\n",
    "Note:\n",
    "\n",
    "* You can use your own one-hot encoding procedure for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Acl1oFZdav2a"
   },
   "outputs": [],
   "source": [
    "one_hot_matrix = torch.eye(D)\n",
    "\n",
    "one_hot = {}\n",
    "cnt = 0\n",
    "for c in characters:\n",
    "    one_hot[c] = one_hot_matrix[cnt]\n",
    "    cnt += 1\n",
    "\n",
    "#Â handling of 0 content\n",
    "one_hot[0] = torch.zeros(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfVfBxauav2a"
   },
   "source": [
    "### Task 3: Sequence Coding\n",
    "\n",
    "Write a function that provides the inputs and targets for a given sequence of the specified sequence length and index.\n",
    "The last value of the target sequence should be the encoding of the character at the given index.\n",
    "If a character is requested from outside the data range, prepend the inputs (and the targets) with 0.\n",
    "Assure that $\\vec t^{\\{s\\}} = \\vec x^{\\{s+1\\}}$ $\\forall s<S$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiQkzUN3av2a"
   },
   "outputs": [],
   "source": [
    "def sequence(index, S):\n",
    "    # collect both input and target encodings\n",
    "    inputs, targets = [], []\n",
    "    # go through the sequence and turn characters into encodings\n",
    "\n",
    "    pos = index-S\n",
    "\n",
    "    for count in range(S):\n",
    "        # position is still away from the target being the first value of the list\n",
    "        if pos < 0:\n",
    "            inputs.append(one_hot[0])\n",
    "            targets.append(one_hot[0])\n",
    "\n",
    "        # the first letter appears to be the first target. However the input is still 0\n",
    "        elif pos == 0:\n",
    "            inputs.append(one_hot[0])\n",
    "            targets.append(one_hot[data[pos]])\n",
    "\n",
    "        # the first letter is now input to the target of the second letter; normal case as long as within dataset\n",
    "        elif pos < len(data):\n",
    "            inputs.append(one_hot[data[pos-1]])\n",
    "            targets.append(one_hot[data[pos]])\n",
    "\n",
    "        # when reached the end of the data, there's an input but no target\n",
    "        elif pos == len(data):\n",
    "            inputs.append(one_hot[data[pos-1]])\n",
    "            targets.append(one_hot[0])\n",
    "\n",
    "        # over the end of the dataset, just zeros again\n",
    "        else:\n",
    "            inputs.append(one_hot[0])\n",
    "            targets.append(one_hot[0])\n",
    "\n",
    "        pos += 1\n",
    "\n",
    "    return torch.stack(inputs), torch.stack(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5jjJzUWav2a"
   },
   "source": [
    "### Test 1: Sequences\n",
    "\n",
    "Get a sequence for size 5 with index 2. This test assures that the data and target vectors are as desired, i.e., the first elements are 0 vectors, and later one-hot encoded data is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TA1QkHHav2b"
   },
   "outputs": [],
   "source": [
    "# get sequence\n",
    "x,t = sequence(2,5)\n",
    "\n",
    "# check prepended zeros\n",
    "assert torch.all(x[:4] == 0)\n",
    "assert torch.all(t[:3] == 0)\n",
    "\n",
    "# check one-hot encoded inputs and targets\n",
    "assert torch.all(torch.sum(x[4:], axis=1) == 1)\n",
    "assert torch.all(torch.sum(t[3:], axis=1) == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfWROc1vav2b"
   },
   "source": [
    "We use the standard data loader with a batch size of $B=256$. Theoretically, each training sample could have its own sequence length $S$. To enable batch processing, the sequence size must be the same for each element in the batch (otherwise it cannot be transformed as one large tensor). Thus, our dataset needs to have a fixed sequence size $S$. An exact value for $S$ can be selected by yourself.\n",
    "\n",
    "### Task 4: Dataset and Data Loader\n",
    "\n",
    "Implement a `Dataset` class derived from `torch.utils.data.Dataset` that provides $\\mathbf X^{[n]}$ and $\\mathbf T^{[n]}$. Implement three functions:\n",
    "\n",
    "1. The constructor `__init__(self, data, S)` that takes the dataset $\\vec c$ and (initial) sequence length $S$.\n",
    "2. The function `__len__(self)` that returns the number of samples in our dataset.\n",
    "3. Finally the index function `__getitem__(self, index)` that returns the sequences $\\mathbf X^{[n]}$ and $\\mathbf T^{[n]}$ for a given `index`. The function from Task 3 can be used for this.\n",
    "\n",
    "After implementing the `Dataset`, instantiate a `DatLoader` for the dataset with batch size of $B=256$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMruOAnZav2b"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, data, S):\n",
    "    # prepare the data as required\n",
    "    self.S = S\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # return input and target value for the given index\n",
    "    return sequence(index, self.S)\n",
    "\n",
    "  def __len__(self):\n",
    "    # return the length of this dataset\n",
    "    return len(data)\n",
    "\n",
    "# instantiate dataset and data loader for a reasonable sequence length S\n",
    "dataset = Dataset(data, 100)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHcwfXFRav2b"
   },
   "source": [
    "### Test 2: Data Sizes\n",
    "\n",
    "Here we check that the shape of the input and target from all batches are appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oIUt-u9yav2b"
   },
   "outputs": [],
   "source": [
    "for x,t in data_loader:\n",
    "  dataset.S = random.choice(range(1,20))\n",
    "  assert x.shape[2] == D\n",
    "  assert x.shape == t.shape\n",
    "  assert torch.all(x[:, 1:, :] == t[:, :-1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMG22l9Hav2b"
   },
   "source": [
    "## Elman Network Implementation\n",
    "\n",
    "While there are implementations for recursive networks available in PyTorch, we here attempt our own implementation of the Elman network.\n",
    "The input to our network is a batch of sequences of dimension $\\mathcal X\\in \\mathbb R^{B\\times S\\times D}$.\n",
    "Our network contains three fully-connected layers with dimensions $\\mathbf W^{(1)} \\in \\mathbb R^{K\\times D}$, $\\mathbf W^{(r)} \\in \\mathbb R^{K\\times K}$ and $\\mathbf W^{2} \\in \\mathbb R^{D\\times K}$ (plus bias neurons as handled by PyTorch).\n",
    "The network processing will iterate through our sequence, and processes all elements in the batch simultaneously.\n",
    "First, the hidden activation $\\mathbf H^{\\{0\\}} \\in \\mathbb R^{B,K}$ is initialized with 0.\n",
    "Then, we iterate over $1\\leq s\\leq S$ to process:\n",
    "\n",
    "$\\mathbf A^{\\{s\\}} = \\mathbf W^{(1)} \\mathbf X^{\\{s\\}} + \\mathbf W^{(r)} \\mathbf H^{\\{s-1\\}}$ $~~~~~~~~~$\n",
    "$\\mathbf H^{\\{s\\}}= g\\bigl(\\mathbf A^{\\{s\\}}\\bigr)$ $~~~~~~~~~$\n",
    "$\\mathbf Z^{\\{s\\}} = \\mathbf W^{(2)} \\mathbf H^{\\{s\\}}$\n",
    "\n",
    "where $g$ is the activation function, `PReLU` in our case, and $\\mathbf X^{\\{s\\}}$ is the data matrix stored as $\\mathcal X_{:,s,:}$. The final output of our network $\\mathcal Z$ is a combination of all $\\mathbf Z^{\\{s\\}}$ matrices in dimension as our input $\\mathcal Z\\in \\mathbb R^{B\\times S\\times D}$.\n",
    "\n",
    "For training, we need to compare the output $\\mathcal Z$ of our network with our target batch $\\mathcal T$. We will make use of the categorical cross-entropy loss as implemented in PyTorch's `torch.nn.CrossEntropyLoss`. In our case, we will implicitly compute:\n",
    "\n",
    "$$\\mathcal J^{CCE} = \\frac1{SB} \\sum\\limits_{b=1}^B \\sum\\limits_{s=1}^S \\sum\\limits_{d=1}^D \\mathcal T_{b,s,d} \\log \\mathcal Y_{b,s,d}$$\n",
    "\n",
    "where $\\mathcal Y_{b,s,d}$ is the result of SoftMax over the dimension $D$, which is the last index of our tensor.\n",
    "As the documentation of `torch.nn.CrossEntropyLoss` states, the SoftMax is always computed across the `second` dimension of the data tensor (which would be $s$ in our case).\n",
    "Hence, we need to make sure to reorder the dimensions of the tensors $\\mathcal X$ and $\\mathcal T$ such that the computation is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SPEV9-Cav2c"
   },
   "source": [
    "### Task 5: Elman Network Implementation\n",
    "\n",
    "Manually implement an Elman network derived from `torch.nn.Module` class using one fully-connected layer for hidden, recurrent and output units.\n",
    "\n",
    "1. In the constructor, instantiate all required layers and activation functions for the given values of $D$ and $K$.\n",
    "2. In the `forward` function, implement the processing of the input in the Elman network. Make sure that logit values are computed and returned for each element in the sequence. Try to use as much tensor processing as possible. Remember the shape of $\\mathcal X$ is $B\\times S\\times D$, and when going through the sequence, we need to process $\\vec x^{\\{s\\}}$ separately, while working on all batch elements simultaneously.\n",
    "\n",
    "\n",
    "Note:\n",
    "\n",
    "* You can also make use of `torch.nn.RNN` Pytorch module to build the recurrent layer and use a fully connected layer on top to implement the Elman network. However, using this module might not be easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3b_Pg2eav2c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assume 'device' is defined, e.g.,\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ElmanNetwork(torch.nn.Module):\n",
    "  def __init__(self, D, K):\n",
    "    super(ElmanNetwork, self).__init__()\n",
    "    # Use nn.Linear for input-to-hidden, recurrent, and hidden-to-output layers\n",
    "    self.fc_in = torch.nn.Linear(D, K)\n",
    "    self.fc_rec = torch.nn.Linear(K, K)\n",
    "    self.fc_out = torch.nn.Linear(K, D)\n",
    "    self.activation = torch.nn.PReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, S, D = x.shape\n",
    "    h_s = torch.zeros(B, self.fc_rec.in_features, device=x.device)\n",
    "    Z = torch.empty(x.shape, device=x.device)\n",
    "\n",
    "    for s in range(S):\n",
    "      x_s = x[:, s, :]\n",
    "      # The logic is the same, but we call the layers\n",
    "      a_s = self.fc_in(x_s) + self.fc_rec(h_s)\n",
    "      h_s = self.activation(a_s)\n",
    "      z = self.fc_out(h_s)\n",
    "      Z[:, s, :] = z\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zye64Q-av2c"
   },
   "source": [
    "### Test 3: Network Output\n",
    "\n",
    "We instantiate an Elman network with $K=1000$ hidden units.\n",
    "We generate test samples in a given format, and forward them through the network and assure that the results are in the required dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBxkRO6bav2c"
   },
   "outputs": [],
   "source": [
    "network = ElmanNetwork(D, 1000).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "  test_input = torch.empty(7, 25, D, device=device)\n",
    "  test_output = network(test_input)\n",
    "  assert test_input.shape == test_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9g6yamSav2c"
   },
   "source": [
    "### Task 6: Training Loop\n",
    "\n",
    "To train the Elman network, we will use categorical cross-entropy loss, averaged over all samples in the sequence.\n",
    "For each batch, we can optionally use a different sequence size -- while the size inside a batch must stay the same.\n",
    "\n",
    "According to the PyTorch documentation, the `CrossEntropyLoss` handles logits and targets in shape $B\\times O\\times\\ldots$.\n",
    "In our case, logits and targets are in dimension $B\\times S\\times O$.\n",
    "Hence, we need to make sure that we re-order the indexes such that we fulfil the requirement; you might want to use the `permute` or `transpose` operator.\n",
    "\n",
    "Instantiate the optimizer with an appropriate learning rate $\\eta$ and the loss function.\n",
    "Implement the training loop for 20 epochs -- more epochs will further improve the results.\n",
    "Compute the average training loss per epoch.\n",
    "\n",
    "Note that 20 epochs will train for about 2 minutes, if implemented in an optimized way, on the GPU. Non-optimized training will take considerably longer.\n",
    "\n",
    "WARNING: `CrossEntropyLoss` will not complain when the index order for the output $\\mathcal Y$ and targets $\\mathcal T$ is incorrect, just the results will be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952,
     "referenced_widgets": [
      "caa154ae2f0c427988f22203829a68bb",
      "813f27ac707549cd99beca04b47da278",
      "c0d072c91c3a41c7bdee9cfede1a8537",
      "0337d758b3a34928928e0c80229662bc",
      "5c76852af747458a805bc680595d052d",
      "d8332c7cf876410ea4070f36e496ec4e",
      "f63b71106b3c4c87b55132a99c110294",
      "efe5db46a4134875924e7048d2edfe81",
      "6e950bc3616045ac80bbf0581f95f7e8",
      "6d47ea214fa841ffbb531c0631245ed2",
      "7ed3f081db6d4655b9ed5ba3a31f7ac3"
     ]
    },
    "id": "LOPnjBW0av2c",
    "outputId": "299bbc20-c581-4d88-c064-b5ea930e2be9"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "D = len(characters)\n",
    "\n",
    "# Instantiate the network and move it to the correct device\n",
    "network = ElmanNetwork(D, 1000).to(device)\n",
    "\n",
    "# Instantiate the Adam optimizer and the loss function\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in tqdm(range(50), desc=\"Epochs\"):\n",
    "  # Reset average loss for the epoch\n",
    "  train_loss = 0.\n",
    "\n",
    "  for x, t in data_loader:\n",
    "    # Move data to the same device as the network\n",
    "    x, t = x.to(device), t.to(device)\n",
    "\n",
    "    # 1. Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 2. Compute network output (forward pass)\n",
    "    z = network(x)\n",
    "\n",
    "    # 3. Compute loss\n",
    "    # Convert the one-hot target 't' to a tensor of class indices\n",
    "    # torch.argmax(t, dim=2) finds the index of the '1' in the last dimension\n",
    "    target_indices = torch.argmax(t, dim=2)\n",
    "\n",
    "    # Calculate the loss using the permuted logits and the correct target format\n",
    "    J = loss(z.permute(0, 2, 1), target_indices)\n",
    "\n",
    "\n",
    "    # 4. Compute gradients for this batch (backward pass)\n",
    "    J.backward()\n",
    "\n",
    "    # 5. Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Accumulate the loss for the epoch\n",
    "    train_loss += J.item()\n",
    "\n",
    "  # print average loss for training\n",
    "  print(f\"\\rEpoch {epoch+1}; train loss: {train_loss/len(data_loader):1.5f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaSEoqd6av2c"
   },
   "source": [
    "## Writing a Poem\n",
    "\n",
    "With the trained network, we will turn it into a poet.\n",
    "Given some initial seed strings, we let the network predict the next character, which we append to our text. We repeat this process until we have produced a given string size.\n",
    "\n",
    "For this purpose, we need to implement three functions.\n",
    "The first function needs to turn a given text into something that the network understands as input.\n",
    "The second function needs to interpret the network output, i.e., it needs to select a character from the predicted logits.\n",
    "There, we can implement two ways:\n",
    "1. We take the character with the highest predicted class:\n",
    "$$o^* = \\argmax_o \\vec y^{\\{S\\}}_o$$\n",
    "2. We can also perform probability sampling, where each of the sample is drawn based on the probability that SoftMax provides -- such a function is for example implemented in `random.choices`.\n",
    "\n",
    "Finally, we need to implement a function to iterstively call the encoding and prediction functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4hTqMjSav2d"
   },
   "source": [
    "### Task 7: Text Encoding\n",
    "\n",
    "For a given text (a sequence of $S$ characters), provide the encoding $\\mathcal X \\in R^{B\\times S\\times D}$.\n",
    "Assure that the batch dimension for $B=1$ is added to the encoding, so that the network is able to handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWhBDIcBav2d"
   },
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "  \"\"\"\n",
    "  Converts a string of text into a one-hot encoded tensor.\n",
    "  \"\"\"\n",
    "  # Create a list of one-hot vectors for each character in the text\n",
    "  encoding_list = [one_hot[c] for c in text]\n",
    "\n",
    "  # Stack the list of vectors into a single tensor of shape (S, D)\n",
    "  stacked_encoding = torch.stack(encoding_list)\n",
    "\n",
    "  # Add a batch dimension to create the final shape (1, S, D)\n",
    "  encoding = stacked_encoding.unsqueeze(0)\n",
    "\n",
    "  return encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13Vc9KBtav2d"
   },
   "source": [
    "### Task 8: Next Element Prediction\n",
    "\n",
    "Write a function that predicts the next character of the sequence based on the logit values obtained from the network.\n",
    "Implement both ways:\n",
    "1. Using the maximum probability, i.e., selecting the character with the highest SoftMax probability $\\max_o z^{\\{S\\}}_o$ and append this character to the `text`.\n",
    "2. Using a probability sampling, i.e., we randomly draw a character based on the SoftMax probability distribution $\\vec y^{\\{S\\}}$. `random.choices` provides the possibility to pass a list of characters and a list of probabilities.\n",
    "\n",
    "Use the Boolean parameter `use_best` of your function to distinguish the two cases.\n",
    "\n",
    "Note:\n",
    "\n",
    "* In our case, we are only interested in the logit for the last element of our sequences, i.e., $\\vec z^{\\{S\\}}$.\n",
    "* The logits are in dimension $\\mathcal Z \\in \\mathbb R^{B\\times S\\times D}$ with $B=1$, and we are generally only interested in the prediction for the last sequence item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpmCLU_Wav2d"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict(z, use_best=True):\n",
    "  \"\"\"\n",
    "  Predicts the next character from a logit tensor.\n",
    "\n",
    "  Args:\n",
    "    z (torch.Tensor): The logit tensor of shape (1, S, D).\n",
    "    use_best (bool): If True, returns the most likely character.\n",
    "                     If False, samples from the probability distribution.\n",
    "\n",
    "  Returns:\n",
    "    str: The predicted next character.\n",
    "  \"\"\"\n",
    "  # Select the logits for the last element in the sequence, shape: (D,)\n",
    "  z_S = z[0, -1, :]\n",
    "\n",
    "  if use_best:\n",
    "    # Find the index of the logit with the highest value\n",
    "    best_index = torch.argmax(z_S).item()\n",
    "    # Look up the character corresponding to that index\n",
    "    next_char = characters[best_index]\n",
    "  else:\n",
    "    # Convert logits to probabilities using the softmax function\n",
    "    probabilities = F.softmax(z_S, dim=0)\n",
    "    # Use random.choices to sample a character based on the probabilities\n",
    "    # We need to move the probabilities to the CPU and convert to a list/numpy array\n",
    "    next_char = random.choices(characters, weights=probabilities.cpu().numpy(), k=1)[0]\n",
    "\n",
    "  return next_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qwH_cw3av2d"
   },
   "source": [
    "### Task 9: Sequence Completion\n",
    "\n",
    "\n",
    "Write a function that takes a `seed` text which it will complete with the given number of characters.\n",
    "Write a loop that turns the current `text` into an encoded sequence of its characters using the function from Task 7.\n",
    "Forward the text through the network and take the prediction of the last sequence item $\\vec z^{\\{S\\}}$ using the function from Task 8.\n",
    "Append this to the current text (remember that Python strings are immutable).\n",
    "Repeat this loop 80 times, and return the resulting `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZKNfPqpav2d"
   },
   "outputs": [],
   "source": [
    "\n",
    "def sequence_completion(seed, count, use_best):\n",
    "  \"\"\"\n",
    "  Completes a given seed text by generating new characters.\n",
    "\n",
    "  Args:\n",
    "    seed (str): The initial text to start with.\n",
    "    count (int): The number of characters to generate.\n",
    "    use_best (bool): The prediction method (True for best, False for sampling).\n",
    "\n",
    "  Returns:\n",
    "    str: The completed text.\n",
    "  \"\"\"\n",
    "  # We start with the given seed\n",
    "  text = seed\n",
    "  # We don't need to track gradients during inference\n",
    "  with torch.no_grad():\n",
    "    # Set the network to evaluation mode\n",
    "    network.eval()\n",
    "    for i in range(count):\n",
    "      # Turn current text to a one-hot encoded batch and move to device\n",
    "      x = encode(text).to(device)\n",
    "\n",
    "      # Get the logits from the network\n",
    "      z = network(x)\n",
    "\n",
    "      # Predict the next character using the logits\n",
    "      next_char = predict(z, use_best=use_best)\n",
    "\n",
    "      # Append the predicted character to the text\n",
    "      text += next_char\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bF3J2mpYav2d"
   },
   "source": [
    "### Task 10: Text Production\n",
    "\n",
    "Select several seeds (such as `\"thi\", \"ba\", \"wea\", \"tru\", \"sum\", \"q\"`) and let the network predict the following 80 most probable characters, or using probability sampling.\n",
    "Write the completed sentences to console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jH0qv_FHav2e",
    "outputId": "1c293906-a544-47b6-bbaf-a0779ce649d9"
   },
   "outputs": [],
   "source": [
    "# A list of seed phrases to start the generation\n",
    "seeds = [\"thi\", \"ba\", \"wea\", \"tru\", \"sum\", \"q\"]\n",
    "\n",
    "print(\"--- Generating Shakespearean Text ---\\n\")\n",
    "for seed in seeds:\n",
    "  # Generate text by always picking the best character\n",
    "  best = sequence_completion(seed, 80, use_best=True)\n",
    "  # Print the seed and the generated text\n",
    "  print(f\"Best:    \\\"{seed}\\\" -> \\\"{best}\\\"\")\n",
    "\n",
    "  # Generate text by sampling from the probability distribution\n",
    "  sampled = sequence_completion(seed, 80, use_best=False)\n",
    "  # Print the seed and the generated text\n",
    "  print(f\"Sampled: \\\"{seed}\\\" -> \\\"{sampled}\\\"\")\n",
    "\n",
    "  # Print a blank line for readability\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "Best:    \"thi\" -> \"thine eyes?*bet fretheisting oren,*fertif it fate thynoler tryiei who terr o'er to \"\n",
    "\n",
    "Sampled: \"thi\" -> \"think on thee from falli on s pentt nitcinr ieti fortious your own love?*the rofe p\"\n",
    "\n",
    "\n",
    "Best:    \"ba\" -> \"bastate,*bey see ses,*reditivant i was bus old,*'  sleeds**n scet nerrims frid to \"\n",
    "\n",
    "Sampled: \"ba\" -> \"back as hiss to disthing my unot be sarred,*for where*that is,*moces descove birie\"\n",
    "\n",
    "\n",
    "Best:    \"wea\" -> \"wear against the bay fftensing siailose faetrgie fr th i that snows that i cover he\"\n",
    "\n",
    "Sampled: \"wea\" -> \"wear against the lamoninearagenit*enu nde tbe t'teeaiendsti  t wi mot crt wow  hovl\"\n",
    "\n",
    "\n",
    "Best:    \"tru\" -> \"true sight,*whth no for thou ly comparse*cleat not, of hits,*wh swael my aitide not\"\n",
    "\n",
    "Sampled: \"tru\" -> \"trust,*and descredied thy oftrimenor can me st hats and praitser meri's sdre i to d\"\n",
    "\n",
    "\n",
    "Best:    \"sum\" -> \"summer's day,*agd 's the gart, forne tipe yl of antuitine s awfe my its are*ang to \"\n",
    "\n",
    "Sampled: \"sum\" -> \"sumand out of dothapting thy break i spridg s afiitaset f enaet t a n hi r enr sofr\"\n",
    "\n",
    "\n",
    "Best:    \"q\" -> \"quickly ste,*let int in need mui rain adletnn ei olh  hrrat wfodensttra g ond itn\"\n",
    "\n",
    "Sampled: \"q\" -> \"quest and moring endore i  be thine minen wink is earthy selfuct on train blgld,*\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bpj-U9rqbrDV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
