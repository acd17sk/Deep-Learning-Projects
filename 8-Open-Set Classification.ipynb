{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsSWtTLDm0Lx"
   },
   "source": [
    "# Assignment 8: Open-Set Classification\n",
    "\n",
    "In this assignment, we develop a network that is capable of correctly classifying known classes while at the same time rejecting unknown samples that occur during inference time.\n",
    "To showcase the capability, we make use of the MNIST dataset that we artificially split into known, negative and unknown classes; this allows us to train a network on the data without requiring too expensive hardware.\n",
    "Known and negative classes are used during training, and unknown classes appear only in the testing set.\n",
    "\n",
    "## Dataset\n",
    "We split the MNIST dataset into 4 known classes, 4 negative classes (used for training) and 2 unknown classes (used only for testing).\n",
    "While several splits might be possible, here we restrict to the following:\n",
    "* Known class indexes: (1, 4, 7, 9)\n",
    "* Negative class indexes: (0, 2, 3, 5)\n",
    "* Unknown class indexes: (6,8)\n",
    "\n",
    "Please note that, in PyTorch, class indexing starts at 0 (other than in the lecture where class indexing starts at 1).\n",
    "\n",
    "We rely on the `torchvision.datasets.MNIST` implementation of the MNIST dataset, which we adapt to our needs.\n",
    "The constructor of our Dataset class takes one parameter that defines the purpose of this dataset (`\"train\", \"validation\", \"test\"`).\n",
    "The `\"train\"` partition uses the training samples of the *known* and the *negative* classes.\n",
    "The `\"validation\"` partition uses the test samples of the *known* and the *negative* classes.\n",
    "Finally, the `\"test\"` partition uses the test samples of the *known* and the *unknown* classes.\n",
    "\n",
    "In our implementation of the Dataset class, we need to implement two functions.\n",
    "* First, the constructor `__init__(self, purpose)` selects the data based on our purpose. \n",
    "* Second, the index function `__getitem__(self, n)` returns a pair $(X^n, \\vec t{\\,}^{n})$ for the sample with the index $n$, where $X \\in \\mathbb R^{1\\times28\\times28}$ with values in range $[0,1]$ and $\\vec t \\in \\mathbb R^{O}$, see below.\n",
    "\n",
    "Since our loss function (cf. Task 5) requires our target vectors to be in vector format, we need to convert the target index $\\tau^n$ into its vector representation $\\vec t{\\,}^n$.\n",
    "Particularly, we need to provide the following target vectors: \n",
    "\n",
    "<center> \n",
    "\n",
    " $\\tau^n = 1 : \\vec t{\\,}^n = (1,0,0,0)$ \n",
    "\n",
    " $\\tau^n = 4 : \\vec t{\\,}^n = (0,1,0,0)$ \n",
    " \n",
    " $\\tau^n = 7 : \\vec t{\\,}^n = (0,0,1,0)$\n",
    " \n",
    " $\\tau^n = 9 : \\vec t{\\,}^n = (0,0,0,1)$\n",
    "\n",
    " else: $\\vec t{\\,}^n = (\\frac14,\\frac14,\\frac14,\\frac14)$\n",
    "\n",
    "</center>\n",
    "\n",
    "\n",
    "### Task 1: Target Vectors\n",
    "\n",
    "Implement a function that generates a target vector for any of the ten different classes according to above description. The return value should be a `torch.tensor` of type float.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKlkqz_ym0L2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# define the three types of classes\n",
    "known_classes = (1,4,7,9)\n",
    "negative_classes = (0,2,3,5)\n",
    "unknown_classes = (6, 8)\n",
    "O = len(known_classes)\n",
    "\n",
    "# define one-hot vectors\n",
    "labels_known = torch.nn.functional.one_hot(torch.tensor([0,1,2,3]), num_classes=4)\n",
    "label_unknown = torch.tensor([1/O,1/O,1/O,1/O])\n",
    "\n",
    "def target_vector(index):\n",
    "  # select correct one-hot vector for known classes, and the 1/O-vectors for unknown classes\n",
    "    if index in known_classes:\n",
    "        return labels_known[known_classes.index(index)]\n",
    "    else:\n",
    "        return label_unknown\n",
    "    \n",
    "for i in range(0,10,1):\n",
    "    print(i, target_vector(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFSme-RNm0L4"
   },
   "source": [
    "### Test 1: Check your Target Vectors\n",
    "\n",
    "Test that your target vectors are correct, for all tpyes of known and unknown samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cn9oEs61m0L4"
   },
   "outputs": [],
   "source": [
    "# check that the target vectors for known classes are correct\n",
    "for index in known_classes:\n",
    "  t = target_vector(index)\n",
    "  print(index, t) \n",
    "  assert max(t) == 1\n",
    "  assert sum(t) == 1\n",
    "\n",
    "# check that the target vectors for negative and unknown classes are correct\n",
    "for index in negative_classes + unknown_classes:\n",
    "  t = target_vector(index)\n",
    "  print(index, t)\n",
    "  assert max(t) == 0.25\n",
    "  assert sum(t) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPmC7414m0L5"
   },
   "source": [
    "### Tasks 2 and 3: Dataset Construction and Dataset Item Selection\n",
    "\n",
    "Write a dataset class that derives from `torchvision.datasets.MNIST` in `PyTorch` and adapts some parts of it. \n",
    "In the constructor, make sure that you let `PyTorch` load the dataset by calling the base class constructor `super` with the desired parameters. Afterward, the `self.data` and `self.targets` are populated with all samples and target indexes.\n",
    "From these, we need to sub-select the samples that fit our current `purpose` and store them back to `self.data` and `self.targets`.\n",
    "\n",
    "Second, we need to implement the index function of our dataset, where we need to return both the image and the target vector.\n",
    "The images in `self.data` were originally stored as `uint8` values in the dimension $\\mathbb N^{N\\times28\\times28}$ with values in $[0, 255]$.\n",
    "The targets in `self.targets` were originally stored as class indexes in the dimension $\\mathbb N^N$. Make sure that you return both in the desired format.\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Since Jupyter Notebook does not allow splitting classes over several code boxes, the two tasks are required to be solved in the same code box.\n",
    "* **The definition below is just one possibility.** There are many ways to implement this dataset interface. \n",
    "* With a clever implementation of the constructor, there is no need to overwrite the `__getitem__(self,index)` function.\n",
    "* Depending on your implementation, you might also need to overwrite the `__len__(self)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sa5iLh2Nm0L5"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DataSet(torchvision.datasets.MNIST):\n",
    "  def __init__(self, purpose=\"train\"):\n",
    "    \"\"\"\n",
    "    Initializes a custom MNIST dataset.\n",
    "    \n",
    "    Args:\n",
    "      purpose (str): One of \"train\", \"validation\", or \"test\".\n",
    "                     Determines which subset of classes to load.\n",
    "    \"\"\"\n",
    "    # Define which classes and which MNIST split (train/test) to use\n",
    "    if purpose == \"train\":\n",
    "        self.valid_classes = list(range(8)) # Digits 0-7\n",
    "        use_train_split = True\n",
    "    elif purpose == \"validation\":\n",
    "        self.valid_classes = [8] # Digit 8\n",
    "        use_train_split = True # Validation data comes from the original training set\n",
    "    elif purpose == \"test\":\n",
    "        self.valid_classes = [9] # Digit 9\n",
    "        use_train_split = False # Test data comes from the original test set\n",
    "    else:\n",
    "        raise ValueError(\"`purpose` must be one of 'train', 'validation', or 'test'\")\n",
    "\n",
    "    # Call the base class constructor to load the appropriate MNIST split\n",
    "    super().__init__(root=\"./data/MNIST\",\n",
    "                     train=use_train_split,\n",
    "                     download=True)\n",
    "\n",
    "    # Filter the data and targets to include only the valid classes\n",
    "    # We create a boolean mask to select the samples.\n",
    "    mask = torch.isin(self.targets, torch.tensor(self.valid_classes))\n",
    "    \n",
    "    # Apply the mask to keep only the desired samples\n",
    "    self.data = self.data[mask]\n",
    "    self.targets = self.targets[mask]\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    \"\"\"\n",
    "    Retrieves and processes a single data sample.\n",
    "\n",
    "    Args:\n",
    "      index (int): The index of the sample to retrieve.\n",
    "\n",
    "    Returns:\n",
    "      tuple: A tuple containing the processed image tensor and the one-hot encoded target tensor.\n",
    "    \"\"\"\n",
    "    # Retrieve the raw image and target index using the provided index\n",
    "    # The parent class __getitem__ would work, but we need to apply transformations.\n",
    "    img, target_idx = self.data[index], self.targets[index]\n",
    "\n",
    "    # Process the image:\n",
    "    # 1. Convert from uint8 [0, 255] to float32 [0.0, 1.0]\n",
    "    # 2. Add a channel dimension: (28, 28) -> (1, 28, 28)\n",
    "    input_tensor = (img.float() / 255.0).unsqueeze(0)\n",
    "\n",
    "    # Process the target:\n",
    "    # Convert the class index into a one-hot encoded vector.\n",
    "    # MNIST has 10 total classes (0-9).\n",
    "    target_tensor = F.one_hot(target_idx, num_classes=10).float()\n",
    "    \n",
    "    return input_tensor, target_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvmK-kmdm0L6"
   },
   "source": [
    "### Test 2: Data Sets\n",
    "\n",
    "\n",
    "Instantiate the training dataset.\n",
    "Implement a data loader for the training dataset with a batch size of 64.\n",
    "Assure that all inputs are of the desired type and shape.\n",
    "Assert that the target values are in the correct format, and the sum of the target values per sample is one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgFrIjoom0L6"
   },
   "outputs": [],
   "source": [
    "# instantiate the training dataset\n",
    "train_set = DataSet(purpose=\"train\")\n",
    "train_loader = torch.utils.data.DataLoader(train_set, 64, shuffle=True)\n",
    "actual_len = len(train_set)\n",
    "print(f\"Length of the filtered training set: {actual_len}\")\n",
    "\n",
    "# assert that we have not filtered out all samples\n",
    "assert 60000 > actual_len > 45000\n",
    "\n",
    "# check the batch and assert valid data and sizes\n",
    "for x,t in train_loader:\n",
    "  assert len(x) <= 64\n",
    "  assert len(t) == len(x)\n",
    "  assert torch.all(torch.sum(t, axis = 1) == 1)\n",
    "  assert x.shape == torch.Size([x.shape[0], 1, 28, 28])\n",
    "  assert x.dtype == torch.float32\n",
    "  assert torch.max(x) <= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W67k2w9Mm0L7"
   },
   "source": [
    "### Task 4: Utility Function\n",
    "\n",
    "Implement a function that splits a batch of samples into known and negative/unknown parts. For the known parts, also provide the target vectors.\n",
    "How can we know which of the data samples are known samples, and which are negative/unknown?\n",
    "\n",
    "This function needs to return three elements:\n",
    "* First, the samples from the batch that belong to known classes.\n",
    "* Second, the target vectors that belong to the known classes.\n",
    "* Finally, the samples from the batch that belong to negative/unknown classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSDfc-2Tm0L7"
   },
   "outputs": [],
   "source": [
    "def split_known_unknown(batch, targets):\n",
    "  \"\"\"\n",
    "  Splits a batch into known and unknown samples.\n",
    "\n",
    "  Args:\n",
    "    batch (torch.Tensor): The input data samples.\n",
    "    targets (torch.Tensor): The one-hot encoded target vectors.\n",
    "\n",
    "  Returns:\n",
    "    tuple: A tuple containing (known_samples, known_targets, unknown_samples).\n",
    "  \"\"\"\n",
    "  # A target is \"known\" if it's a one-hot vector, meaning its elements sum to 1.\n",
    "  # An \"unknown\" target is a zero vector, summing to 0.\n",
    "  target_sums = torch.sum(targets, dim=1)\n",
    "  \n",
    "  # Create boolean masks to select the indexes\n",
    "  known = (target_sums == 1)\n",
    "  unknown = (target_sums == 0)\n",
    "  \n",
    "  # Return the sliced tensors based on the masks\n",
    "  return batch[known], targets[known], batch[unknown]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVq2US6am0L8"
   },
   "source": [
    "## Loss Function and Confidence\n",
    "\n",
    "We write our own PyTorch implementation of our loss function.\n",
    "Particularly, we implement a manual way to define the derivative of our loss function via `torch.autograd.Function`, which allows us to define the forward and backward pass on our own.\n",
    "For this purpose, we need to implement two `static` functions in our loss.\n",
    "The function `forward(ctx, logits, targets)` is required to compute the loss value and allows us to store some variables in the context of the backward pass.\n",
    "The `backward(ctx, result)` provides us with the result of the forward function (the loss value) as well as the context with our stored variables.\n",
    "Here, we need to compute the derivative of the loss with respect to both of the inputs to the forward function (which might look a bit confusing), i.e.,$\\frac{\\partial \\mathbf{J}^{CCE}}{\\partial \\mathbf{Z}}$ and $\\frac{\\partial \\mathbf{J}^{CCE}}{\\partial \\mathbf{T}}$.\n",
    "Since the latter is not required, we can also simply return `None` for the second derivative.\n",
    "\n",
    "<font color=#FF000>Hint: if you think the implementation of loss function is too hard, you can also cross-entropy as your loss function (**since PyTorch version 1.11**).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QT58YXiE4_C2"
   },
   "source": [
    "### Task 5: Loss Function Implementation\n",
    "\n",
    "Implement a `torch.autograd.Function` class for the adapted SoftMax function according to the equations provided in the lecture.\n",
    "You might want to compute the log of the network output $\\ln y_o$ from the logits $z_o$ via `torch.nn.functional.log_softmax`.\n",
    "Store all the data required for the backward pass in the context during `forward`, and extract these from the context during `backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czmU2Aqqm0L8"
   },
   "outputs": [],
   "source": [
    "class AdaptedSoftMax(torch.autograd.Function):\n",
    "\n",
    "  # implement the forward propagation\n",
    "  @staticmethod\n",
    "  def forward(ctx, logits, targets):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for the cross-entropy loss.\n",
    "    \"\"\"\n",
    "    # compute the log probabilities via log_softmax\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    \n",
    "    # We need the softmax probabilities (y) and targets (t) for the backward pass,\n",
    "    # as the gradient is (y - t).\n",
    "    probs = torch.exp(log_probs)\n",
    "    ctx.save_for_backward(probs, targets)\n",
    "    \n",
    "    # Compute the cross-entropy loss, averaged over the batch.\n",
    "    # J = - (1/N) * sum(targets * log_probs)\n",
    "    loss = -torch.sum(targets * log_probs) / logits.shape[0]\n",
    "    return loss\n",
    "\n",
    "  # implement Jacobian (backward pass)\n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_output):\n",
    "    \"\"\"\n",
    "    Computes the backward pass, correctly applying the chain rule.\n",
    "    \"\"\"\n",
    "    probs, targets = ctx.saved_tensors\n",
    "    batch_size = probs.shape[0]\n",
    "    \n",
    "    # This is the local gradient of the loss w.r.t. the logits\n",
    "    local_grad = (probs - targets) / batch_size\n",
    "    \n",
    "    # The final gradient is the local gradient multiplied by the upstream gradient\n",
    "    # (grad_output). This correctly implements the chain rule.\n",
    "    dJ_dy = local_grad * grad_output\n",
    "    \n",
    "    return dJ_dy, None\n",
    "\n",
    "# DO NOT REMOVE!\n",
    "# here we set the adapted softmax function to be used later\n",
    "adapted_softmax = AdaptedSoftMax.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eopp5YC6m0L9"
   },
   "source": [
    "### Task 5a: Alternative Loss Function\n",
    "\n",
    "In case the loss function is too difficult to implement, you can also choose to rely on PyTorch's automatic gradient computation and simply define your loss function without the backward pass.\n",
    "\n",
    "In this case, we only need to define the forward pass. A simple function `adapted_softmax(logits, targets)` is sufficient.\n",
    "\n",
    "You can implement any variant of the categorical cross-entropy loss function on top of SoftMax activations as defined in the lecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFhw66Pxm0L9"
   },
   "outputs": [],
   "source": [
    "def adapted_softmax_alt(logits, targets):\n",
    "  \"\"\"\n",
    "  Computes cross-entropy loss using PyTorch's autograd.\n",
    "  \"\"\"\n",
    "  # Compute the log probabilities (numerically stable)\n",
    "  log_probs = F.log_softmax(logits, dim=1)\n",
    "  \n",
    "  # Compute the cross-entropy loss, averaged over the batch.\n",
    "  # PyTorch will automatically compute the gradient for this computation.\n",
    "  loss = -torch.sum(targets * log_probs) / logits.shape[0]\n",
    "  \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDwhq72Vm0L9"
   },
   "source": [
    "### Task 6: Confidence Evaluation\n",
    "\n",
    "\n",
    "Implement a function to compute the confidence value for a given batch of samples. \n",
    "Compute Softmax confidence and split these confidences between known and negative/unknown classes. \n",
    "For samples from known classes, sum up the SoftMax confidences of the correct class. \n",
    "For negative/unknown samples, sum 1 minus the maximum confidence for any of the known classes; also apply the $\\frac1O$ correction for the minimum possible SoftMax confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ooydMNsm0L9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def confidence(logits, targets):\n",
    "    \"\"\"\n",
    "    Compute confidence values for a batch of samples.\n",
    "    - Known samples: sum of softmax confidence at the correct class.\n",
    "    - Unknown samples: sum of (1 - max softmax confidence) + 1/O correction.\n",
    "    \"\"\"\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    O = logits.size(1)\n",
    "\n",
    "    # Identify known vs unknown\n",
    "    is_known = (targets.max(dim=1).values == 1.0)\n",
    "    is_unknown = ~is_known\n",
    "\n",
    "    # --- Known confidence ---\n",
    "    conf_known = torch.sum(torch.sum(probs * targets, dim=1)[is_known])\n",
    "\n",
    "    # --- Unknown confidence ---\n",
    "    conf_unknown = torch.tensor(0.0, device=logits.device)\n",
    "    if is_unknown.any():\n",
    "        max_conf, _ = torch.max(probs[is_unknown], dim=1)\n",
    "        conf_unknown = torch.sum(1.0 - max_conf + 1.0 / O)\n",
    "\n",
    "    return conf_known + conf_unknown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e543YdEvm0L-"
   },
   "source": [
    "### Test 3: Check Confidence Implementation\n",
    "\n",
    "Test that your confidence implementation does what it is supposed to do.\n",
    "\n",
    "Note that confidence values should always be between 0 and 1, other values indicate an issue in the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXAruuB0m0L-"
   },
   "outputs": [],
   "source": [
    "# select good logit vectors for known and unknown classes\n",
    "logits = torch.tensor([[10., 0., 0., 0.], [-10., 0, -10., -10.], [0.,0.,0.,0.]])\n",
    "# select the according target vectors for these classes\n",
    "targets = torch.stack([target_vector(known_classes[0]), target_vector(known_classes[1]), target_vector(negative_classes[0])])\n",
    "\n",
    "\n",
    "# the confidence should be close to 1 for all cases\n",
    "assert 3 - confidence(logits, targets) < 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWFgh2Nwm0L-"
   },
   "source": [
    "## Network and Training\n",
    "\n",
    "We make use of the same convolutional network as utilized in Assignment 6, to which we append a final fully-connected layer with $K$ inputs and $O$ outputs.\n",
    "Additionally, we replace the $\\tanh$ activation function with the better-performing ReLU function.\n",
    "\n",
    "The topology can be found in the following:\n",
    "1. 2D convolutional layer with $Q_1$ channels, kernel size $5\\times5$, stride 1 and padding 2\n",
    "2. 2D maximum pooling layer with kernel size $2\\times2$ and stride 2\n",
    "3. Activation function ReLU\n",
    "4. 2D convolutional layer with $Q_2$ channels, kernel size $5\\times5$, stride 1 and padding 2\n",
    "5. 2D maximum pooling layer with kernel size $2\\times2$ and stride 2\n",
    "6. Activation function ReLU\n",
    "7. Flatten layer to convert the convolution output into a vector\n",
    "8. Fully-connected layer with the correct number of inputs and $K$ outputs\n",
    "9. Fully-connected layer with $K$ inputs and $O$ outputs\n",
    "\n",
    "However, instead of relying on the `torch.nn.Sequential` class, we need to define our own network class, which we need to derive from `torch.nn.Module` -- since our network has two outputs.\n",
    "We basically need to implement two methods in our network.\n",
    "The constructor `__init__(self, Q1, Q2, K)` needs to call the base class constructor and initialize all required layers of our network.\n",
    "The `forward(self, x)` function then passes the input through all of our layers and returns both the deep features (extracted at the first fully-connected layer) and the logits (extracted from the second fully-connected layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1J9AXd55YO4"
   },
   "source": [
    "\n",
    "### Task 7: Network Definition\n",
    "\n",
    "We define our own small-scale network to classify known and unknown samples for MNIST.\n",
    "We basically use the same convolutional network as in Assignment 6, with some small adaptations.\n",
    "However, this time we need to implement our own network model since we need to modify our network output.\n",
    "\n",
    "Implement a network class, including the layers as provided above.\n",
    "Implement both the constructor and the forward function.\n",
    "Instantiate the network with $Q_1=16, Q_2=32, K=20, O=4$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYyPEmsWm0L-"
   },
   "outputs": [],
   "source": [
    "class Network (torch.nn.Module):\n",
    "  def __init__(self, Q1, Q2, K, O):\n",
    "    # call base class constructor\n",
    "    super(Network, self).__init__()\n",
    "\n",
    "    # --- Layer Definitions ---\n",
    "    # Input for MNIST is 1 channel. Output is Q1 channels.\n",
    "    # Padding=2 with Kernel=5 and Stride=1 preserves the image dimensions (28x28).\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=Q1, kernel_size=5, stride=1, padding=2)\n",
    "    # Input is Q1 channels from conv1. Output is Q2 channels.\n",
    "    self.conv2 = nn.Conv2d(in_channels=Q1, out_channels=Q2, kernel_size=5, stride=1, padding=2)\n",
    "    \n",
    "    # Pooling and activation functions can be defined once and re-used\n",
    "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # Halves the dimensions\n",
    "    self.act = nn.ReLU()\n",
    "    self.flatten = nn.Flatten()\n",
    "    \n",
    "    # After two 2x2 pooling layers, a 28x28 image becomes 7x7.\n",
    "    # The number of input features to the first FC layer is channels * height * width.\n",
    "    fc1_in_features = Q2 * 7 * 7\n",
    "    self.fc1 = nn.Linear(in_features=fc1_in_features, out_features=K)\n",
    "    self.fc2 = nn.Linear(in_features=K, out_features=O)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # --- Forward Pass ---\n",
    "    # compute first block of convolution, pooling and activation\n",
    "    # (N, 1, 28, 28) -> (N, Q1, 28, 28) -> (N, Q1, 14, 14) -> (N, Q1, 14, 14)\n",
    "    a = self.act(self.pool(self.conv1(x)))\n",
    "    \n",
    "    # compute second block of convolution, pooling and activation\n",
    "    # (N, Q1, 14, 14) -> (N, Q2, 14, 14) -> (N, Q2, 7, 7) -> (N, Q2, 7, 7)\n",
    "    a = self.act(self.pool(self.conv2(a)))\n",
    "    \n",
    "    # Flatten the output for the fully-connected layers\n",
    "    a_flat = self.flatten(a)\n",
    "    \n",
    "    # get the deep features as the output of the first fully-connected layer\n",
    "    deep_features = self.fc1(a_flat)\n",
    "    \n",
    "    # get the logits as the output of the second fully-connected layer\n",
    "    logits = self.fc2(deep_features)\n",
    "    \n",
    "    # return both the logits and the deep features\n",
    "    return logits, deep_features\n",
    "\n",
    "# --- Network Instantiation ---\n",
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# create network with specified dimensions\n",
    "network = Network(Q1=16, Q2=32, K=20, O=4).to(device)\n",
    "\n",
    "# You can print the network to verify its structure\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rxnrxtRm0L-"
   },
   "source": [
    "### Task 8: Training Loop\n",
    "\n",
    "Instantiate the training and validation set and according data loaders.\n",
    "Instantiate an SGD optimizer with an appropriate learning rate (the optimal learning rate might depend on your loss function implementation and can vary between 0.1 and 0.00001).\n",
    "Implement the training and validation loop for 10 epochs (you can also train for 100 epochs if you want).\n",
    "Compute the training set confidence during the epoch.\n",
    "At the end of each epoch, also compute the validation set confidence measure.\n",
    "Print both the training set and validation set confidence scores to the console.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnLIrcTzm0L_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "batch_size = 64\n",
    "learning_rate = 0.01   # adjust if loss diverges\n",
    "epochs = 10\n",
    "\n",
    "# --- Datasets & Dataloaders ---\n",
    "train_set = DataSet(purpose=\"train\")\n",
    "validation_set = DataSet(purpose=\"validation\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# --- Network & Optimizer ---\n",
    "network = Network(Q1=16, Q2=32, K=20, O=4).to(device)\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()  # use softmax-crossentropy for classification\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(epochs):\n",
    "    network.train()\n",
    "    train_conf, validation_conf = 0.0, 0.0\n",
    "\n",
    "    # Training phase\n",
    "    for x, t in train_loader:\n",
    "        x, t = x.to(device), t.to(device)\n",
    "\n",
    "        # forward\n",
    "        logits, features = network(x)\n",
    "\n",
    "        # compute loss\n",
    "        # targets may be one-hot, so convert to class indices for CrossEntropyLoss\n",
    "        if t.ndim > 1:\n",
    "            targets_idx = torch.argmax(t, dim=1)\n",
    "        else:\n",
    "            targets_idx = t\n",
    "        loss = loss_fn(logits, targets_idx)\n",
    "\n",
    "        # backward + update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate confidence\n",
    "        train_conf += confidence(logits.detach(), t).item()\n",
    "\n",
    "    # Validation phase\n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, t in validation_loader:\n",
    "            x, t = x.to(device), t.to(device)\n",
    "            logits, features = network(x)\n",
    "            validation_conf += confidence(logits, t).item()\n",
    "\n",
    "    # normalize by dataset sizes\n",
    "    train_conf /= len(train_set)\n",
    "    validation_conf /= len(validation_set)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}; \"\n",
    "          f\"train confidence: {train_conf:.5f}, \"\n",
    "          f\"val confidence: {validation_conf:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDqRH6Bsm0L_"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "For evaluation, we test two different things.\n",
    "First, we check whether our intuition is correct, and the training helps reduce the deep feature magnitudes of unknown samples while maintaining magnitudes for known samples.\n",
    "It is also interesting to see whether there is a difference between samples of the negative classes that were seen during training, and unknown classes that were not.\n",
    "For this purpose, we extract the deep features for the validation and test sets, compute their magnitudes, and plot them in a histogram.\n",
    "\n",
    "The second evaluation computes Correct Classification Rates (CCR) and False Positive Rates (FPR) for a given confidence threshold $\\zeta=0.98$ (based on your training results, you might want to vary this threshold).\n",
    "For the known samples, we compute how often the correct class was classified with a confidence over threshold.\n",
    "For unknown samples, we assess how often one of the known classes was predicted with a confidence larger than $\\zeta$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAhXp8zh7ent"
   },
   "source": [
    "### Task 9: Feature Magnitude Plot\n",
    "\n",
    "Extract deep features for validation and test set samples and compute their magnitudes. Split them into known, negative (validation set), and unknown (test set). Plot a histogram for each of the three types of samples.\n",
    "Note that the minimum magnitude is 0, and the maximum magnitude can depend on your network training success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iM0SayyKm0L_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Instantiate test set and dataloader ---\n",
    "test_set = DataSet(purpose=\"test\")\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# --- Collect feature magnitudes ---\n",
    "known, negative, unknown = [], [], []\n",
    "\n",
    "network.eval()\n",
    "with torch.no_grad():\n",
    "    # Validation set → contains known + negative\n",
    "    for x, t in validation_loader:\n",
    "        x, t = x.to(device), t.to(device)\n",
    "\n",
    "        logits, deep_features = network(x)\n",
    "        norms = torch.norm(deep_features, p=2, dim=1)\n",
    "\n",
    "        # Split into known vs negative\n",
    "        is_known = (t.max(dim=1).values == 1.0)\n",
    "        is_negative = ~is_known   # validation negatives\n",
    "\n",
    "        known.extend(norms[is_known].cpu().tolist())\n",
    "        negative.extend(norms[is_negative].cpu().tolist())\n",
    "\n",
    "    # Test set → contains known + unknown\n",
    "    for x, t in test_loader:\n",
    "        x, t = x.to(device), t.to(device)\n",
    "\n",
    "        logits, deep_features = network(x)\n",
    "        norms = torch.norm(deep_features, p=2, dim=1)\n",
    "\n",
    "        # Split into known vs unknown\n",
    "        is_known = (t.max(dim=1).values == 1.0)\n",
    "        is_unknown = ~is_known\n",
    "\n",
    "        known.extend(norms[is_known].cpu().tolist())\n",
    "        unknown.extend(norms[is_unknown].cpu().tolist())\n",
    "\n",
    "# --- Plot histograms ---\n",
    "plt.figure(figsize=(6,4))\n",
    "max_mag = 20  # adjust depending on training results\n",
    "\n",
    "plt.hist(known, bins=100, range=(0,max_mag), density=True,\n",
    "         color=\"g\", histtype=\"step\", label=\"Known\")\n",
    "plt.hist(negative, bins=100, range=(0,max_mag), density=True,\n",
    "         color=\"b\", histtype=\"step\", label=\"Negative\")\n",
    "plt.hist(unknown, bins=100, range=(0,max_mag), density=True,\n",
    "         color=\"r\", histtype=\"step\", label=\"Unknown\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Deep Feature Magnitude\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Feature Magnitudes: Known vs Negative vs Unknown\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTSMfeJ_m0L_"
   },
   "source": [
    "### Task 10: Classification Evaluation\n",
    "\n",
    "For a fixed threshold of $\\zeta=0.98$, compute CCR and FPR for the test set.\n",
    "A well-trained network can achieve a CCR of > 90% for an FPR < 10%.\n",
    "You might need to vary the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlrBZlHxm0L_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "zeta = 0.98\n",
    "\n",
    "correct, known = 0, 0\n",
    "false, unknown = 0, 0\n",
    "\n",
    "network.eval()\n",
    "with torch.no_grad():\n",
    "    for x, t in test_loader:\n",
    "        x, t = x.to(device), t.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        logits, deep_features = network(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        # predicted class and confidence\n",
    "        max_conf, pred_class = torch.max(probs, dim=1)\n",
    "\n",
    "        # split known vs unknown (same trick as before)\n",
    "        is_known = (t.max(dim=1).values == 1.0)\n",
    "        is_unknown = ~is_known\n",
    "\n",
    "        # --- Known samples ---\n",
    "        if is_known.any():\n",
    "            targets_idx = torch.argmax(t[is_known], dim=1)\n",
    "            preds = pred_class[is_known]\n",
    "            confs = max_conf[is_known]\n",
    "\n",
    "            # correct if predicted class = target and confidence >= zeta\n",
    "            correct += torch.sum((preds == targets_idx) & (confs >= zeta)).item()\n",
    "            known += len(targets_idx)\n",
    "\n",
    "        # --- Unknown samples ---\n",
    "        if is_unknown.any():\n",
    "            confs = max_conf[is_unknown]\n",
    "\n",
    "            # false positives = accepted as known if confidence >= zeta\n",
    "            false += torch.sum(confs >= zeta).item()\n",
    "            unknown += len(confs)\n",
    "\n",
    "# --- Report results ---\n",
    "print(f\"CCR: {correct} of {known} = {correct/known*100:.2f}%\")\n",
    "print(f\"FPR: {false} of {unknown} = {false/unknown*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
