{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QI6JM_kLdEvG"
   },
   "source": [
    "# Assignment 1: Perceptron Learning\n",
    "\n",
    "The goal of this exercise is to apply the perceptron learning to a total of $N=100$ automatically generated, separable random data $X=\\bigl\\{\\vec x^1, \\vec x^2,\\ldots,\\vec x^ N \\bigr\\}$ with each $\\vec x^n = \\bigl(x_1^n, x_2^n\\bigr)^T$.\n",
    "Each data point $\\vec x^n$ is accompanied by an according target value $X=\\bigl\\{t^1, t^2,\\ldots,t^ N\\bigr\\}$ with $t^n \\in \\{-1,+1\\}$.\n",
    "\n",
    "## Data Generation\n",
    "The data should be generated such that\n",
    "$\\forall n\\leq\\frac N2\\colon \\vec x^n \\sim \\mathcal N_{\\vec\\mu_+, \\sigma_+}$.\n",
    "These samples will be our positive data labeled with $t^n=1$.\n",
    "Similarly, we generate our negative data with\n",
    "$\\forall n>\\frac N2\\colon \\vec x^n \\sim \\mathcal N_{\\vec\\mu_-, \\sigma_-}$\n",
    "and label them as $t^n=-1$.\n",
    "\n",
    "### Task 1: Data Samples\n",
    "\n",
    "Given the number of samples and the means (mu) and standard deviations (sigma) of positive (pos) and negative (neg) data, generate and return data samples including their labels. Remember to add the bias neuron $x_0=1$ to each of the samples.\n",
    "\n",
    "Hints:\n",
    "1. Use `numpy` package to generate data.\n",
    "2. Exemplary means could be selected as: $\\vec\\mu_+=(-5,3)^T$ and $\\vec\\mu_- = (5, -3)^T$. The standard deviations $\\sigma_+$ and $\\sigma_-$ should be selected such that the data is most likely separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "azZboNhUdEvI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def expand_with_ones(X):\n",
    "    # add a column of ones to the front of the input matrix\n",
    "        \n",
    "    X_out = np.c_[np.ones(X.shape[0]) , X ]            \n",
    "    return X_out\n",
    "\n",
    "def dataset(number_of_samples, mu_pos, sigma_pos, mu_neg, sigma_neg):\n",
    "    n_of_samples_per_category = int(number_of_samples/2)\n",
    "    \n",
    "    # create positive and negative data\n",
    "    positive_data = np.random.default_rng().normal(mu_pos, sigma_pos, size=(n_of_samples_per_category, 2))\n",
    "    negative_data = np.random.default_rng().normal(mu_neg, sigma_neg, size=(n_of_samples_per_category, 2))\n",
    "\n",
    "    # assign positive and negative labels\n",
    "    positive_labels = np.ones(50)\n",
    "    negative_labels = np.ones(50) * -1\n",
    "\n",
    "    # concatenate positive and negative data\n",
    "    all_data = np.concatenate((positive_data, negative_data), axis=0)\n",
    "    all_labels = np.concatenate((positive_labels, negative_labels), axis=0)\n",
    "\n",
    "    # anything else to consider?\n",
    "    \n",
    "    # add the bias column to the data\n",
    "    all_data = expand_with_ones(all_data)\n",
    "\n",
    "    # return both X and T\n",
    "    return all_data, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8Pr9LqddEvJ"
   },
   "source": [
    "### Task 2: Select Data Parameters and Line Parameters\n",
    "\n",
    "We want to select data points such that we exactly know where the ideal separating line should be placed.\n",
    "Note that data samples are not always separable since they are generated randomly.\n",
    "You should determine, which means and standard deviations are useful.\n",
    "\n",
    "Once you have defined your means, you should also define the separating line.\n",
    "The easiest is to provide it as Cartesian equation: $w_0 + w_1 x_1 + w_2 x_2$.\n",
    "Note that the separating line is orthogonal to the vector $\\overrightarrow{\\vec\\mu_- \\vec\\mu_+}$, that the normal of the line $(w_1, w_2)^T$ is orthogonal to the line, and that $w_0$ should be selected such that the line $\\vec w$ is in the middle of $\\vec\\mu_+$ and $\\vec\\mu_-$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "x-T3HqCskEx5"
   },
   "outputs": [],
   "source": [
    "def least_squares_compute_parameters(X_input, y):\n",
    "\n",
    "\n",
    "    # compute the parameters based on the expanded X and y using the least-squares method    \n",
    "    w = np.linalg.lstsq(X,y, rcond=None)[0]\n",
    "    \n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "kNTieS-wdEvJ",
    "outputId": "a20e4d5b-9196-43e1-c1c4-cca81499a3d2"
   },
   "outputs": [],
   "source": [
    "X, T = dataset(100,  mu_pos=[5,-3], sigma_pos=1.2, mu_neg=[-3,5], sigma_neg=0.5)\n",
    "\n",
    "w_manual = least_squares_compute_parameters(X, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqoFxw5TdEvK"
   },
   "source": [
    "### Test 1: Linear Separability Test\n",
    "\n",
    "A line $a = f_{\\vec w}(\\vec x) = w_0 + w_1 x_1 + w_2 x_2$ linearly separates the data $(X,T)$ if $\\forall n: a^{[n]} t^{[n]} > 0$ for $a^{[n]} = f_{\\vec w}(\\vec x^{[n]})$.\n",
    "The below function implements this linear separability test. We apply this test to your data $(X,T)$ from Task 1 and your manually selected line $\\vec w$ from Task 2 to assure that the line separates the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hzBu7-1LdEvK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data is separated by the manually selected line\n"
     ]
    }
   ],
   "source": [
    "def separability_test(X, T, w):\n",
    "    res = np.dot(X,w) * T > 0\n",
    "    return np.all(res)\n",
    "\n",
    "# Test 1: check that the weights are separating the data\n",
    "if separability_test(X, T, w_manual):\n",
    "    print(\"The data is separated by the manually selected line\")\n",
    "else:\n",
    "    print(\"The anually selected line does not separate the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogfiOhgsdEvK"
   },
   "source": [
    "### Task 3: Perceptron\n",
    "\n",
    "The perceptron is defined as the Adaline $$a = f_{\\vec w}(\\vec x)$$ that is thresholded using the sign function $$\\mathrm{sign}(a) = \\begin{cases} +1 &\\text{if } a \\geq 0\\\\ -1 & \\text{otherwise.}\\end{cases}$$\n",
    "Implement a function that computes and returns the perceptron for a given data point $\\vec x$ and line parameters $\\vec w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Qd0lIRoOdEvK"
   },
   "outputs": [],
   "source": [
    "def perceptron(x, w):\n",
    "    h = np.dot(x,w)\n",
    "    \n",
    "    y_ = np.where(h >= 0.0, 1, -1)\n",
    "    return y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pg5xHzIdEvL"
   },
   "source": [
    "## Perceptron Learning\n",
    "\n",
    "The perceptron learning rule is defined as follows.\n",
    "First, the weights $\\vec w = (w_0, w_1, w_2)^T$ is initialized randomly.\n",
    "Then, for each sample $(x,t)$ of the dataset we check if the sample is correctly classified as $H(f_{\\vec w}(\\vec x)) t > 0$.\n",
    "If the sample is classified incorrectly, the weights are adapted: $w_0 = w_0 + t$, $w_1 = w_1 + tx_1$, $w_2 = w_2 + tx_2$.\n",
    "This step is repeated until all samples are classified correctly.\n",
    "\n",
    "\n",
    "### Task 4: Perceptron Learning Implementation\n",
    "\n",
    "Implement a function that performs perceptron learning for a given dataset $(X,T)$ and a given initial weight vector $\\vec w$.\n",
    "The final weight vector $\\vec w^*$ shall be returned from that function.\n",
    "Define a proper stopping criterion for the iteration.\n",
    "Consider in your implementation error cases that could arise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FpTM4KeldEvL"
   },
   "outputs": [],
   "source": [
    "def perceptron_learning(X, T, w):\n",
    "    # first, make a copy of your weights\n",
    "    \n",
    "    w_star = w.copy()\n",
    "\n",
    "\n",
    "    # then, iterate over the data and perform perceptron learning\n",
    "    for x, t in list(zip(X, T)):\n",
    "        y_ = perceptron(x, w_star)\n",
    "        \n",
    "        if y_ * t < 0:\n",
    "            w_star = w_star + t * x\n",
    "\n",
    "    # finally, return the optimal weights\n",
    "    return w_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcC6pSYadEvL"
   },
   "source": [
    "### Test 2: Sanity Check\n",
    "\n",
    "We call the perceptron learning function with the data from task 1 and the manual line from task 2. If the line separates the data, it should not be changed. Here we test if this is the actual outcome of the perceptron learning function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WieC4bXFdEvL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As desired, perceptron learning does not optimize an already separating line\n"
     ]
    }
   ],
   "source": [
    "w_star = perceptron_learning(X, T, w_manual)\n",
    "\n",
    "# check if the output is as expected\n",
    "if np.any(w_manual != w_star):\n",
    "    print(\"Warning: the perceptron algorithm seems to be wrong\")\n",
    "else:\n",
    "    print(\"As desired, perceptron learning does not optimize an already separating line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuBVcexsdEvM"
   },
   "source": [
    "### Task 5: Weight Initialization\n",
    "\n",
    "Implement a function that generates and returns randomly initialized weights $\\vec w \\in [-1,1]^3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XgXy_i35dEvM"
   },
   "outputs": [],
   "source": [
    "def random_weights(input_size, output_size, lowest = -1, highest = 1):\n",
    "    # The Glorot normal initializer, also called Xavier normal initializer\n",
    "    w = np.random.uniform(lowest, highest, (output_size, input_size))\n",
    "    w *= np.sqrt(1/w.shape[1]) * 0.01\n",
    "    return w.reshape((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6tEKxHPdEvM"
   },
   "source": [
    "### Task 6: Perceptron Learning Execution\n",
    "\n",
    "Call the perceptron learning function with the data from task 1 and the randomly generated initial weight vector from task 5.\n",
    "Store the resulting weight vector $\\vec w^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "o177OywjdEvM"
   },
   "outputs": [],
   "source": [
    "# create random weights\n",
    "w_initial = random_weights(X.shape[1], 1)\n",
    "\n",
    "# perform perceptron learning\n",
    "w_star = perceptron_learning(X, T, w_initial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1kE4CbFdEvM"
   },
   "source": [
    "### Test 3: Result Validation\n",
    "\n",
    "We verify that the optimized $\\vec w^*$ actually separates the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NLfAQxEddEvN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data is separated by the optimal line\n"
     ]
    }
   ],
   "source": [
    "# verify that we have learned to separate the data\n",
    "if separability_test(X, T, w_star):\n",
    "    print(\"The data is separated by the optimal line\")\n",
    "else:\n",
    "    print(\"The optimal line does not separate the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBVKqAPjdEvN"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "We have selected our data to be 2-dimensional to be able to visualize the results.\n",
    "For this purpose, we would like to jointly plot the positive and the negative data from Task 1 together with the decision boundaries of the weight vectors obtained in Tasks 2 and 6.\n",
    "An example can be found in the exercise slides.\n",
    "\n",
    "### Task 7: Plotting\n",
    "\n",
    "First, we need to plot the data points such that positive data are plotted with green dots, and negative data with red dots.\n",
    "\n",
    "Then, we need to compute the line parameters. For this purpose, we define the separating line in Cartesian coordinates $f_{\\vec w}(\\vec x) = 0$ and solve it to the parametric form $x_2 = \\beta x_1 + \\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qlOADapdEvN",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def line_parameters(w):\n",
    "  # compute parametric line parameters from Cartesian coordinates\n",
    "  beta = ...\n",
    "  gamma = ...\n",
    "  return beta, gamma\n",
    "\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# create a square plot\n",
    "pyplot.figure(figsize=(8,8))\n",
    "\n",
    "# plot the positive data points\n",
    "pyplot.plot(..., ..., \"g.\", label=\"positive data\")\n",
    "# plot the negative data points\n",
    "pyplot.plot(..., ..., \"r.\", label=\"negative data\")\n",
    "\n",
    "# define positions where to evaluate the line:\n",
    "x1 = ...\n",
    "\n",
    "# compute line parameters for manual line\n",
    "beta, gamma = line_parameters(w_manual)\n",
    "# now, compute the values according to our parametric form:\n",
    "x2 = beta * x1 + gamma\n",
    "# plot lines (might need to call this function twice for the two lines)\n",
    "pyplot.plot(x1, x2, \"m-\", label=\"manual line\")\n",
    "\n",
    "# compute line parameters for optimized line\n",
    "beta, gamma = line_parameters(w_star)\n",
    "# now, compute the values according to our parametric form:\n",
    "x2 = beta * x1 + gamma\n",
    "# plot lines (might need to call this function twice for the two lines)\n",
    "pyplot.plot(x1, x2, \"b-\", label=\"optimized line\")\n",
    "\n",
    "# make the plot more beautiful\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a29cabff5744fce69e08a959ab87b9e77a9f67b498d08783caa8c3bb16f23a00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
