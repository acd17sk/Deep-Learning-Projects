{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DKZj8jK_Dry"
   },
   "source": [
    "# Multi-Output Networks and Batch Processing\n",
    "\n",
    "\n",
    "The goal of this exercise is to get to know some regularization techniques when implementing deep learning methods.\n",
    "For this purpose, we select a dataset that contains data in different formats, some binary ($x_d \\in \\{-1,1\\}$) and some numerical ($x_d\\in \\mathbb N$); and some are categorical, which we ignore for now.\n",
    "As target values, this dataset contains three numerical outputs, so, $\\vec t \\in \\mathbb R^3$ for each sample.\n",
    "These target values should be approximated with a two-layer multi-output network that we will train with the $\\mathcal J^{L_2}$ loss.\n",
    "\n",
    "\n",
    "\n",
    "Remember to make use of `numpy` in the matrix calculation, e.g. `numpy.dot`, `numpy.exp`, `numpy.mean` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lrQLreklXtS"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset of our choice is the Student Performance estimation dataset that was collected in Portugal in two different schools and with two different subjects, i.e., math and Portuguese (the mother tongue).\n",
    "The dataset contains many different inputs such as a binary representation of the school, gender, family sizes, and alike, as well as numerical representations of age, travel time, and alcohol consumption.\n",
    "The dataset also includes some categorical data, which we skip in this assignment.\n",
    "See https://archive.ics.uci.edu/ml/datasets/Student+Performance for more information on the dataset.\n",
    "As a start, we will rely on the Portuguese performance (`\"por\"`), but you can also try to use the Math samples (`\"mat\"`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSol7VoalR-7"
   },
   "source": [
    "### Task 1: Dataset Loading\n",
    "\n",
    "\n",
    "Load the dataset from files and provide the input matrix $\\mathbf X \\in \\mathbb R^{(D+1)\\times N}$ and the output matrix $\\mathbf T \\in \\mathbb R^{O\\times N}$.\n",
    "\n",
    "Due to the difficulty of the task, most of the implementation is provided.\n",
    "The implementation is very literal and, therefore, hopefully readable, while maybe not the most efficient.\n",
    "\n",
    "We skip categorical inputs (indexes 8-11) for now.\n",
    "All other entries are converted either into binary $(-1,1)$ or into an integer range $(0,1,\\ldots)$.\n",
    "The three outputs range between 0 and 20 each. The bias value for $x_0=1$ is also already included.\n",
    "You just need to make sure that the data $(X,T)$ is returned in the desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "316rpDmAsu1P"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wE3DlW-e_Dr1"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "# Dataset origin: https://archive.ics.uci.edu/ml/datasets/Student+Performance\n",
    "\n",
    "def dataset(course=\"por\"):\n",
    "  # load dataset and provide input and target data\n",
    "  # possible data files are \"mat\" and \"por\"\n",
    "\n",
    "  # download data file from URL\n",
    "  dataset_zip_file = \"data/student.zip\"\n",
    "  if not os.path.exists(dataset_zip_file):\n",
    "    import urllib.request\n",
    "    urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00320/student.zip\", dataset_zip_file)\n",
    "    print (\"Downloaded datafile\", dataset_zip_file)\n",
    "\n",
    "  import zipfile\n",
    "  import csv\n",
    "  import io\n",
    "\n",
    "  # collect inputs\n",
    "  inputs = []\n",
    "  targets = []\n",
    "  # some default values: yes=1, no=-1\n",
    "  yn = {\"yes\":1.,\"no\":-1.}\n",
    "  # read through dataset (without actually unzippiung to a file):\n",
    "  # ... open zip file\n",
    "  zip = zipfile.ZipFile(dataset_zip_file)\n",
    "  # ... open data file inside of zip file and convert bytes to text\n",
    "  datafile = io.TextIOWrapper(zip.open(os.path.join(F\"student-{course}.csv\"), 'r'))\n",
    "  # ... read through the lines via CSV reader, using the correct delimited\n",
    "  reader = csv.reader(datafile, delimiter=\";\")\n",
    "  # ... skip header line\n",
    "  next(reader)\n",
    "  for splits in reader:\n",
    "    # read input values\n",
    "    inputs.append([\n",
    "      1.,                             #### BIAS ####\n",
    "      {\"GP\":1.,\"MS\":-1.}[splits[0]],  # school\n",
    "      {\"M\":1.,\"F\":-1.}[splits[1]],    # gender\n",
    "      float(splits[2]),               # age\n",
    "      {\"U\":1.,\"R\":-1.}[splits[3]],    # address\n",
    "      {\"LE3\":1.,\"GT3\":-1.}[splits[4]],# family size\n",
    "      {\"T\":1.,\"A\":-1.}[splits[5]],    # parents living together\n",
    "      float(splits[6]),               # mother education\n",
    "      float(splits[7]),               # father education\n",
    "      # skip categorical values\n",
    "      float(splits[12]),              # travel time\n",
    "      float(splits[13]),              # study time\n",
    "      float(splits[14]),              # failures\n",
    "      yn[splits[15]],                 # extra support\n",
    "      yn[splits[16]],                 # family support\n",
    "      yn[splits[17]],                 # paid support\n",
    "      yn[splits[18]],                 # activities\n",
    "      yn[splits[19]],                 # nursery school\n",
    "      yn[splits[20]],                 # higher education\n",
    "      yn[splits[21]],                 # internet\n",
    "      yn[splits[22]],                 # romantic\n",
    "      float(splits[23]),              # family relation\n",
    "      float(splits[24]),              # free time\n",
    "      float(splits[25]),              # going out\n",
    "      float(splits[26]),              # workday alcohol\n",
    "      float(splits[27]),              # weekend alcohol\n",
    "      float(splits[28]),              # health\n",
    "      float(splits[29]),              # absences\n",
    "    ])\n",
    "\n",
    "    # read targets values\n",
    "    targets.append([\n",
    "      float(splits[30]),              # grade for primary school\n",
    "      float(splits[31]),              # grade for secondary school\n",
    "      float(splits[32]),              # grade for tertiary school\n",
    "    ])\n",
    "\n",
    "  print(F\"Loaded dataset with {len(targets)} samples\")\n",
    "  return numpy.array(inputs).T, numpy.array(targets).T / 20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWNwt2Rb_Dr3"
   },
   "source": [
    "### Test 1: Assert Valid Outputs\n",
    "\n",
    "This test will check the dimension of the loaded dataset, i.e. $\\mathbf X\\in \\mathbb R^{(D+1)\\times N}$ and $\\mathbf T \\in \\mathbb R^{O\\times N}$, and also assure that all target data is in the range $t\\in[0,20]$.\n",
    "\n",
    "Please make sure that your implementation can pass these tests before moving to the next task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jaCpzc-l_Dr3",
    "outputId": "b60d886b-fce7-4f95-97f3-67e1c969e823"
   },
   "outputs": [],
   "source": [
    "X, T = dataset(\"por\")\n",
    "\n",
    "assert numpy.all(T >= 0) and numpy.all(T <= 20)\n",
    "\n",
    "assert X.shape[0] == 27\n",
    "assert T.shape[0] == 3\n",
    "assert T.shape[1] == X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfk_P5eG_Dr3"
   },
   "source": [
    "### Task 2: Input Data Standardization\n",
    "\n",
    "Since the data is in different input regimes, we want to standardize the data.\n",
    "For this purpose, we need to compute the mean and the standard deviation of the data for each input dimension.\n",
    "Then, we implement a function to perform the standardization of the data using the previously computed mean and standard deviation. Make sure that you handle the bias neuron $x_0$ correctly.\n",
    "\n",
    "Please note that `numpy` has all the functionality that you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bUpUAHR_Dr3"
   },
   "outputs": [],
   "source": [
    "# compute mean and standard deviation over dataset\n",
    "mean = numpy.mean(X[1:, :], axis=1)\n",
    "std = numpy.std(X[1:, :], axis=1)\n",
    "# assure to handle x_0 correctly\n",
    "mean = numpy.concatenate(([0], mean))\n",
    "std = numpy.concatenate(([1], std))\n",
    "\n",
    "pre_X = copy.deepcopy(X)\n",
    "\n",
    "def standardize(x, mean, std):\n",
    "  # standardize the given data with the given mean and standard deviation\n",
    "  return (x - mean[:,None]) / std[:,None]\n",
    "\n",
    "# standardize our dataset\n",
    "X = standardize(X, mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MZ_MPc-_Dr4"
   },
   "source": [
    "### Task 3: Batch Processing\n",
    "\n",
    "In order to run stochastic gradient descent, we need to split our dataset into batches of a certain batch size $B$. Implement a function that turns the dataset $(X,T)$ into batches of a certain batch size $B$.\n",
    "Implement this function as a generator function, i.e., use ``yield`` instead of ``return``.\n",
    "Circulate the dataset afresh when all data is consumed, and shuffle the data in each epoch.\n",
    "Make sure that you yield both the input batch and the target batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_v0(X, T, batch_size=16):\n",
    "    # get number of samples\n",
    "    n_samples = X.shape[1]\n",
    "    #number of batches in total\n",
    "    n_batches = math.ceil(n_samples/batch_size)\n",
    "    \n",
    "    while True:\n",
    "        # shuffle\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        \n",
    "        # c is current batch\n",
    "        for c, i in enumerate(range(0,n_samples, batch_size)):\n",
    "            j = indices[i:i+batch_size]\n",
    "            X_batch = X[:,j]\n",
    "            T_batch = T[:,j]\n",
    "\n",
    "            # yield mini-batch inputs, targets, boolean checking if last batch\n",
    "            yield X_batch, T_batch, c == n_batches-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(X, T, batch_size=16):\n",
    "    # get number of samples\n",
    "    n_samples = X.shape[1]\n",
    "    #number of batches in total\n",
    "    n_batches = math.ceil(n_samples/batch_size)\n",
    "    divisible = True\n",
    "    \n",
    "    if n_samples % batch_size!= 0:\n",
    "        k = n_samples % batch_size\n",
    "        rest = batch_size - k \n",
    "        divisible = False\n",
    "\n",
    "    while True:\n",
    "        # shuffle\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        \n",
    "        # c is current batch\n",
    "        for c, i in enumerate(range(0,n_samples, batch_size)):\n",
    "            j = indices[i:i+batch_size]\n",
    "            X_batch = X[:,j]\n",
    "            T_batch = T[:,j]\n",
    "\n",
    "            if not(divisible) and c == n_batches-1:\n",
    "                X_batch = numpy.append(X_batch, X[:,indices[0:rest]],axis=1)\n",
    "                T_batch = numpy.append(T_batch, T[:,indices[0:rest]], axis=1)\n",
    "\n",
    "            # yield mini-batch inputs, targets, boolean checking if last batch\n",
    "            yield X_batch, T_batch, c == n_batches-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgBsBOsa_Dr4"
   },
   "source": [
    "### Test 2: Test your Batches\n",
    "\n",
    "This test is to assure that your batch generation function works as expected. \n",
    "We define some test data for this purpose.\n",
    "The code below checks whether your batch function returns batches with correct content, i.e., $(\\vec x, \\vec t)$-alignment. \n",
    "It also checks that the batches are in the correct dimensions, i.e., that $\\mathbf X \\in \\mathbb R^{(D+1)\\times B}$ and $\\mathbf T \\in \\mathbb R^{O\\times B}$.\n",
    "\n",
    "Make sure you can pass this test before moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "fFndxbf1_CmR",
    "outputId": "206e7d31-e432-4bbf-ba81-0091d99d76b4"
   },
   "outputs": [],
   "source": [
    "XX = numpy.array([[i] * 5 for i in range(50)]).T\n",
    "TT = numpy.array([[i] for i in range(10,60)]).T\n",
    "\n",
    "batch_size = 16\n",
    "remainder = np.remainder(XX.shape[1], batch_size)\n",
    "remainder = batch_size if remainder == 0 else remainder\n",
    "print(f\"The last batch should have size: {remainder}\")\n",
    "\n",
    "for counter, (x,t,last_batch) in enumerate(batch_v0(XX, TT, batch_size)):\n",
    "    # Test 2 was modified to:\n",
    "    # if last batch then the last batch should have size:\n",
    "    # Remainder of Dataset_size / batch_size\n",
    "    # e.g., remainder of 50/16 is 2\n",
    "    # so last batch should have size of 2\n",
    "    current_batch_size = remainder if last_batch else batch_size\n",
    "    \n",
    "    assert x.shape[0] == 5\n",
    "    assert t.shape[0] == 1\n",
    "    assert x.shape[1] == current_batch_size\n",
    "    assert t.shape[1] == current_batch_size\n",
    "    assert numpy.all(x == t-10)\n",
    "\n",
    "    if counter == 20: \n",
    "        print('Test passed!')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = numpy.array([[i] * 5 for i in range(50)]).T\n",
    "TT = numpy.array([[i] for i in range(10,60)]).T\n",
    "\n",
    "for counter, (x,t,batch_size) in enumerate(batch(XX, TT, 16)):\n",
    "  assert x.shape[0] == 5\n",
    "  assert x.shape[1] == 16\n",
    "  assert t.shape[0] == 1\n",
    "  assert t.shape[1] == 16\n",
    "  assert numpy.all(x == t-10)\n",
    "  if counter == 20: \n",
    "    print('Test passed!')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0B8o_mE1l6mp"
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "To train a two-layer multi-output regression network, we need to implement some functions.\n",
    "The network output is computed in three steps:\n",
    "\n",
    "  * Compute network activation for a batch of inputs $\\mathbf X$: $\\mathbf A = \\mathbf W^{(1)}\\mathbf X$\n",
    "  * Call the activation function element-wise: $\\mathbf H = g(\\mathbf A)$. Here, we rely on the logistic activation function $\\sigma$. Assure that the hidden neuron bias $\\mathbf H_{0,:}$ is set appropriately.\n",
    "  * Compute the output $\\mathbf Y$ of the batch: $\\mathbf Y = \\mathbf W^{(2)}\\mathbf H$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pGCwC0L_Dr4"
   },
   "source": [
    "### Task 4: Multi-Output Network\n",
    "\n",
    "Implement a multi-target network that computes the output matrix $\\mathbf Y$ for a given input dataset/batch $\\mathbf X$ and given parameters $\\Theta=(\\mathbf W^{(1)}, \\mathbf W^{(2)})$ using `numpy` operations. \n",
    "The function should return both the output $\\mathbf Y$ and the output of the hidden units $\\mathbf H$ since we will need these in gradient descent. Select the logistic function $\\sigma$ as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1gErN4z_Dr4"
   },
   "outputs": [],
   "source": [
    "def network(X, Theta):\n",
    "    W1, W2 = Theta\n",
    "\n",
    "    # compute activation\n",
    "    A = np.matmul(W1, X)\n",
    "\n",
    "    # compute hidden unit output\n",
    "    H = 1 / (1 + np.exp(-A))\n",
    "    H = np.vstack((np.ones((1, H.shape[1])), H))\n",
    "\n",
    "    # compute network output\n",
    "    Y = np.matmul(W2, H)\n",
    "\n",
    "    return Y, H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zt0CMab4_Dr5"
   },
   "source": [
    "### Task 5: Loss Implementation\n",
    "\n",
    "Implement a loss function that returns the squared loss $\\mathcal J^{L_2} = \\frac1B \\|\\mathbf Y - \\mathbf T\\|_F^2$ for given network outputs $\\mathbf Y$ and target values $\\mathbf T$.\n",
    "Use `numpy` or `scipy` functionality for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSwKiqIc_Dr5"
   },
   "outputs": [],
   "source": [
    "def loss(Y, T):\n",
    "\n",
    "    J = 1/Y.shape[1]*np.square(np.linalg.norm(Y-T, axis=0))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fD2srCKN_Dr5"
   },
   "source": [
    "### Task 6: Gradient Implementation\n",
    "\n",
    "Implement a function that computes and returns the gradient for a given batch $(\\mathbf X, \\mathbf T)$, the given network outputs $\\mathbf Y$ and $\\mathbf H$ as well as current parameters $\\Theta=(\\mathbf W^{(1)}, \\mathbf W^{(2)})$.\n",
    "Make sure to compute the gradient with respect to both weight matrices. Remember that we have used $\\sigma$ as the activation function.\n",
    "Implement the function using the fast version provided in the lecture and make use of `numpy` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(inp):\n",
    "    \n",
    "    return np.multiply(inp, 1 - inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qe8QYVnP_Dr5"
   },
   "outputs": [],
   "source": [
    "def gradient(X, T, Y, H, Theta):\n",
    "    \n",
    "    W1, W2 = Theta\n",
    "\n",
    "    # Backpropagation: output layer -> hidden layer\n",
    "    delta2 = Y-T\n",
    "    # second layer gradient\n",
    "    g2 = np.dot(2/X.shape[1], np.matmul(delta2, H.T))\n",
    "    \n",
    "    # Backpropagation: hidden layer -> input layer\n",
    "    delta1 = np.multiply(sigmoid_derivative(H), np.matmul(W2.T, delta2))\n",
    "    # first layer gradient    \n",
    "    g1 = np.dot(2/X.shape[1], np.matmul(delta1, X.T))\n",
    "\n",
    "    return g1, g2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpeCOHbE_Dr5"
   },
   "source": [
    "### Task 7: Iterative Gradient Descent\n",
    "\n",
    "\n",
    "Implement gradient descent for a given number of 10'000 epochs (**not batches!**) using given initial parameters $\\Theta$ and a given batch size $B$, as well as a learning rate of $\\eta=0.001$.\n",
    "\n",
    "Make use of the standardized dataset from Task 2, split into batches with the function from Task 3, the network from Task 4, the loss from Task 5, and the gradient from Task 6.\n",
    "\n",
    "Make sure that the network output $\\mathbf Y$ and the hidden unit output $\\mathbf H$ are computed only once for each batch. After applying gradient descent, add an option to use momentum learning with the given parameter `mu`.\n",
    "At the end of each epoch, compute and store the loss values for each batch in a list, and this list will be returned at the end.\n",
    "\n",
    "How many iterations do we need when $B < N$? How can you know whether your current batch is the last one of the current epoch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnuFqkg-82l2"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, T, Theta, B, eta=0.001, mu=0.0):\n",
    "    loss_values = []\n",
    "\n",
    "    max_epochs = 10000\n",
    "    Theta_diff = [0., 0.]\n",
    "#     max_epochs = 2\n",
    "\n",
    "    epoch_tqdm = tqdm(range(max_epochs))\n",
    "    for _ in epoch_tqdm:\n",
    "        # iterate over batches\n",
    "        epoch_loss = []\n",
    "        for x,t,c in batch_v0(X, T, batch_size=B):\n",
    "            # compute network output\n",
    "            y, h = network(x, Theta)\n",
    "            \n",
    "            # compute and append loss\n",
    "            loss_signal = loss(y, t)\n",
    "\n",
    "            epoch_loss.append(np.mean(loss_signal))\n",
    "\n",
    "            # compute gradient\n",
    "            grad = gradient(x, t, y, h, Theta)\n",
    "            \n",
    "            # and apply gradient descent + momentum\n",
    "            newTheta = [Theta[0] - np.dot(eta, grad[0][1:]) + mu * Theta_diff[0],\n",
    "                        Theta[1] - np.dot(eta, grad[1]) + mu * Theta_diff[1]]\n",
    "            \n",
    "            # momentum update\n",
    "            Theta_diff = [newTheta[0] - Theta[0],\n",
    "                          newTheta[1] - Theta[1]]\n",
    "            \n",
    "            # save new weights\n",
    "            Theta = newTheta\n",
    "\n",
    "            if c: \n",
    "                loss_values.append(np.mean(epoch_loss))\n",
    "                break\n",
    "                \n",
    "\n",
    "    # return the obtained loss values at the end\n",
    "    return loss_values, Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIvoljpW_Dr5"
   },
   "source": [
    "### Task 8: Run Gradient Descent\n",
    "\n",
    "Select an appropriate number of hidden neurons $K$.\n",
    "Instantiate the weight matrices $\\Theta=(\\mathbf W^{(1)}, \\mathbf W^{(2)})$ using the Xavier method as introduced in the lecture.\n",
    "\n",
    "Run the gradient descent three times, first as normal gradient descent, second as stochastic gradient descent with batch size $B=16$, and third with the same setup as the second but with momentum learning involved, select $\\mu =0.9$.\n",
    "\n",
    "How can you achieve this without requiring separate implementations of the ``gradient_descent`` function from Task 7?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_weights(input_size, output_size, lowest = -1, highest = 1):\n",
    "    # The Glorot normal initializer, also called Xavier normal initializer\n",
    "    s = np.sqrt(1/input_size)\n",
    "    w = np.random.uniform(-s, s, (output_size, input_size))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4BnX1m8_Dr6"
   },
   "outputs": [],
   "source": [
    "K = 200\n",
    "\n",
    "W1 = xavier_weights(X.shape[0],K)\n",
    "W2 = xavier_weights(K+1, T.shape[0])\n",
    "Theta = [W1, W2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gradient descent with full dataset\n",
    "Theta1 = copy.deepcopy(Theta)\n",
    "GD, _ = gradient_descent(X, T, Theta1, B=1, eta=0.001, mu=0.)\n",
    "with open('results/MultiOutput-BatchProcessing Results/GD.npy', 'wb') as f:\n",
    "    np.save(f, GD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run stochastic gradient descent with batches of size 16\n",
    "Theta2 = copy.deepcopy(Theta)\n",
    "SGD, _ = gradient_descent(X, T, Theta2, B=16, eta=0.001, mu=0.)\n",
    "with open('results/MultiOutput-BatchProcessing Results/SGD.npy', 'wb') as f:\n",
    "    np.save(f, SGD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run stochastic gradient descent with batches of size 16 and momentum mu=0.9\n",
    "Theta3 = copy.deepcopy(Theta)\n",
    "SGD_Mo, _ = gradient_descent(X, T, Theta3, B=16, eta=0.001, mu=0.9)\n",
    "with open('results/MultiOutput-BatchProcessing Results/SGD_Mo.npy', 'wb') as f:\n",
    "    np.save(f, SGD_Mo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mi5Wt88CnsNB"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Finally, we want to evaluate how the learning process went and what the network has actually learned.\n",
    "For the former, we will plot the loss values obtained during training.\n",
    "For the latter, we define one specific sample of our own, and we evaluate the impact of several factors on the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1CGUw7k_Dr6"
   },
   "source": [
    "### Task 9: Plotting Loss Progression\n",
    "\n",
    "To show the learning process of the networks, plot the loss values of the three gradient descent steps from Task 8 together into one plot.\n",
    "Do we need to take care of something when plotting both together?\n",
    "\n",
    "Use logarithmic axes wherever you see fit.\n",
    "An exemplary loss progression plot can be found in the slides.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load resutls\n",
    "with open('results/MultiOutput-BatchProcessing Results/GD.npy', 'rb') as f:\n",
    "    GD = np.load(f)\n",
    "\n",
    "with open('results/MultiOutput-BatchProcessing Results/SGD.npy', 'rb') as f:\n",
    "    SGD = np.load(f)\n",
    "\n",
    "with open('results/MultiOutput-BatchProcessing Results/SGD_Mo.npy', 'rb') as f:\n",
    "    SGD_Mo = np.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMFMvuRg_Dr6"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.figure(figsize=(10,6))\n",
    "pyplot.plot(SGD, \"b-\", label=\"Stochastic Gradient Descent\")\n",
    "pyplot.plot(SGD_Mo, \"r-\", label=\"Stochastic Gradient Descent with Momentum\")\n",
    "pyplot.plot(GD, \"g-\", label=\"Gradient Descent\")\n",
    "pyplot.loglog()\n",
    "pyplot.xlabel('Epoch')\n",
    "pyplot.ylabel('Loss')\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDJp99mK_Dr6"
   },
   "source": [
    "### Task 10: Example Evaluation (optional)\n",
    "\n",
    "We want to see what the network has learned.\n",
    "Therefore, we evaluate some data points that would represent a typical Swiss student (except for the school entry, where we select one of them randomly).\n",
    "You can select a specific example, but you can also imagine a student.\n",
    "Please refer to https://archive.ics.uci.edu/ml/datasets/Student+Performance on possible values and the implementation in Tasks 1 and 2 on how to generate an input sample $\\vec x$ for our network. Also, remember that input data need to be standardized before feeding it to the network. \n",
    "\n",
    "Compute the scores that your student would likely get by asking the network, using the parameters $\\Theta$ optimized with stochastic gradient descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta3 = copy.deepcopy(Theta)\n",
    "_, out_theta = gradient_descent(X, T, Theta3, B=16, eta=0.001, mu=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a specific example\n",
    "example = pre_X[:,1].reshape((-1,1))\n",
    "\n",
    "print(example.reshape(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RPCifeO_Dr6"
   },
   "outputs": [],
   "source": [
    "# compute network output\n",
    "prediction, _ = network(standardize(example, mean, std), out_theta)\n",
    "print(\"Prediction :\\t\", prediction.reshape(-1) * 20)\n",
    "print(\"Target :\\t\", T[:,1] * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lB1b-8V_Dr6"
   },
   "source": [
    "### Task 11: Influence of Data Dimensions (optional)\n",
    "\n",
    "\n",
    "For some dimensions in the input feature $\\vec x$, we want to test how different input values for this dimension would influence the outcome.\n",
    "Particularly, we test:\n",
    "\n",
    "  * Gender at index $d=2$: change between male ($1$) and female ($-1$)\n",
    "  * Weekly study time at index $d=10$: vary in the range $[1,4]$ \n",
    "  * Past Failures at index $d=11$: vary in range $[0,3]$ \n",
    "  * Additional classes at index $d=14$: change between yes ($1$) and no ($-1$)\n",
    "  * Romantic relations at index $d=19$: change between yes ($1$) and no ($-1$)\n",
    "  * Weekday alcohol consumption at index $d=23$: varies in the range $[1,6]$.\n",
    "\n",
    "Note that the indexes include the fact that we are omitting some input dimensions, so they might differ from what is listed on the webpage.\n",
    "\n",
    "Did you expect this output?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buTufJpd_Dr6"
   },
   "outputs": [],
   "source": [
    "# implement a way to modify the input at a given index with certain values\n",
    "# and to predict and print the network output for this modification\n",
    "features2test = {\n",
    "    'Gender': {'index': 2, 'val': [1,-1]},\n",
    "    'Weekly study time': {'index': 10, 'val': list(range(1,5))},\n",
    "    'Past Failures': {'index': 11, 'val': list(range(4))},\n",
    "    'Additional classes': {'index': 14, 'val': [1,-1]},\n",
    "    'Romantic relations': {'index': 19, 'val': [1,-1]},\n",
    "    'Weekday alcohol consumption': {'index': 23, 'val': list(range(1,7))}\n",
    "}\n",
    "# 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours\n",
    "value2label = {'Gender': {1: 'Male', -1: 'Female'},\n",
    "               'Weekly study time': {1: '< 2 hours', 2: '2 - 5 hours', 3: '5 - 10 hours', 4: '> 10 hours'},\n",
    "               'Past Failures': {i:f'{i} failure(s)' for i in list(range(4))},\n",
    "               'Additional classes': {1: 'Y', -1: 'N'},\n",
    "               'Romantic relations': {1: 'Y', -1: 'N'},\n",
    "               'Weekday alcohol consumption': {1: 'Very low', 2: 'Low', 3: 'Moderate low', 4:'Moderate high',5: 'High', 6: 'Very high'}\n",
    "              }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this with the 6 modifications and their according to values as seen above\n",
    "for k, v in features2test.items():\n",
    "    print(f'For {k}:')\n",
    "    for value in v['val']:\n",
    "        c_example = example.copy()\n",
    "        c_example.reshape(-1)[v['index']] = value\n",
    "        prediction, _ = network(standardize(c_example, mean, std), out_theta)\n",
    "        print(f'Value: \\t{value2label[k][value]}')\n",
    "        print(\"Prediction :\\t\", prediction.reshape(-1) * 20)\n",
    "        print('--')\n",
    "    print('==========================================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
